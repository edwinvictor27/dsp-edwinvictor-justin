{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d392c71-4f5b-411f-a852-e7c3464a9900",
   "metadata": {},
   "source": [
    "# **Installing Pre-requisites for our Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa9bf0c-052a-4e5d-9f7b-613a6f051d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (2.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pyarrow in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pyarrow) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8986c1be-e617-418d-860c-60a18b195435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9351599-8e89-49ae-a8c5-a78c1e166347",
   "metadata": {},
   "source": [
    "# **MODEL BUILDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a842db1b-1e48-4846-b25e-b9659b6a7466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv file using read_csv function\n",
    "dataset_for_training = r\"C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/data/train.csv\"\n",
    "Training_dataset = pd.read_csv(dataset_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2c66e5-fe71-463a-ab17-fdbd97a7c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_FEATURES = ['LotArea', 'GrLivArea', 'Neighborhood', 'HouseStyle'] # 2 continuos and 2 Categorical features\n",
    "target_featur = ['SalePrice']\n",
    "continuos_datatype_features = ['LotArea', 'GrLivArea']\n",
    "discrete_datatype_features = ['Neighborhood', 'HouseStyle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99ba5222-fddc-48cc-bb8f-0a5ef764b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(data: pd.DataFrame) -> dict[str, str]:\n",
    "\n",
    "## Training_set\n",
    "    \n",
    "    # 1) splitting the dataset\n",
    "    \n",
    "    X = Training_dataset.drop(target_feature, axis=1)\n",
    "    y = Training_dataset[target_feature]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=50)\n",
    "\n",
    "\n",
    "    # 2) Extracting the features from training set\n",
    "    \n",
    "    Extracted_Selected_Features_For_Training = X_train[selected_features]\n",
    "    Extracted_Target_Attribute = y_train[target_feature]\n",
    "    Training_Features = pd.concat([Extracted_Selected_Features_For_Training,y_train], axis=1)\n",
    "\n",
    "    # 3) Encoding the categorical columns from training set\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output= False)\n",
    "    encoder.fit(Training_Features[discrete_datatype_features])\n",
    "    Training_encoded_categories = encoder.transform(Training_Features[discrete_datatype_features])\n",
    "    encoded_discrete_features_training_df = pd.DataFrame(Training_encoded_categories, columns=encoder.get_feature_names_out(discrete_datatype_features))\n",
    "\n",
    "    # 4) Scaling the continuos columns from training set\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[continuos_datatype_features])\n",
    "    scaled_continuos_features_training_df = scaler.transform(X_train[continuos_datatype_features])\n",
    "\n",
    "    # 5) Concatenating the processed training set\n",
    "    \n",
    "    training_continuous_features_df = pd.DataFrame(scaled_continuos_features_training_df , columns= continuos_datatype_features)\n",
    "    Processed_Training_Df = pd.concat([training_continuous_features_df, encoded_discrete_features_training_df] , axis=1)\n",
    "\n",
    "    # 6) Fitting the model\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(Processed_Training_Df, y_train)\n",
    "\n",
    "## Testing_set   \n",
    "\n",
    "    # 1) Extracting the features from testing set\n",
    "    \n",
    "    Extracted_Features_Testing = X_test[selected_features]\n",
    "    Testing_Features = pd.concat([Extracted_Features_Testing,y_test], axis=1)\n",
    "\n",
    "    # 2) Encoding the categorical columns from testing set\n",
    "    \n",
    "    encoder.fit(Testing_Features[discrete_datatype_features])\n",
    "    Testing_encoded_categories = encoder.transform(Testing_Features[discrete_datatype_features])\n",
    "    encoded_discrete_features_testing_df = pd.DataFrame(Testing_encoded_categories, columns=encoder.get_feature_names_out(discrete_datatype_features))\n",
    "\n",
    "    # 3) Scaling the continuos columns from testing set\n",
    "    \n",
    "    scaler.fit(X_test[continuos_datatype_features])\n",
    "    scaled_continuos_features_testing_df = scaler.transform(X_test[continuos_datatype_features])\n",
    "\n",
    "    # 4) Concatenating the processed testing set\n",
    "    \n",
    "    testing_continuous_features_df = pd.DataFrame(scaled_continuos_features_testing_df , columns= continuos_datatype_features)\n",
    "    Processed_Testing_Df = pd.concat([testing_continuous_features_df, encoded_discrete_features_testing_df] , axis=1)\n",
    "    \n",
    "    # 5) Making prediction \n",
    "    \n",
    "    y_pred = model.predict(Processed_Testing_Df)\n",
    "\n",
    "    # 6) Evaluating the model\n",
    "    \n",
    "    Rmsle = np.sqrt(mean_squared_log_error(y_test, y_pred)) \n",
    "    return {'Root Mean Squared Error out of': str(Rmsle) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7318ab31-aa6c-499f-90a3-4bd408746d17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (949, 1), indices imply (949, 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTraining_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[58], line 31\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     29\u001b[0m encoder\u001b[38;5;241m.\u001b[39mfit(Training_Features[discrete_datatype_features])\n\u001b[0;32m     30\u001b[0m Training_encoded_categories \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(Training_Features[discrete_datatype_features])\n\u001b[1;32m---> 31\u001b[0m encoded_discrete_features_training_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTraining_encoded_categories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscrete_datatype_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 4) Scaling the continuos columns from training set\u001b[39;00m\n\u001b[0;32m     35\u001b[0m scaler\u001b[38;5;241m.\u001b[39mfit(X_train[continuos_datatype_features])\n",
      "File \u001b[1;32mD:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages\\pandas\\core\\frame.py:867\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    859\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m             arrays,\n\u001b[0;32m    861\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 867\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    876\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    877\u001b[0m         {},\n\u001b[0;32m    878\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    882\u001b[0m     )\n",
      "File \u001b[1;32mD:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mD:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (949, 1), indices imply (949, 33)"
     ]
    }
   ],
   "source": [
    "build_model(Training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e8f0de-e8b3-475a-911b-c23ae7d8384e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Training_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m build_model(\u001b[43mTraining_dataset\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Training_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "build_model(Training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9dfaded-3493-4689-ac3e-29fa185a0e7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTraining_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(data: pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m## Training_set\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# 1) splitting the dataset\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     X \u001b[38;5;241m=\u001b[39m Training_dataset\u001b[38;5;241m.\u001b[39mdrop(\u001b[43mtarget_feature\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m     y \u001b[38;5;241m=\u001b[39m Training_dataset[target_feature]\n\u001b[0;32m     11\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.35\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target_feature' is not defined"
     ]
    }
   ],
   "source": [
    "build_model(Training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87b2e349-dac1-4804-9a14-90bcce4bd05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Root Mean Squared Error out of': '0.19357831076666'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_model(Training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d01444-eca5-402b-bd8a-aaa36bfb829c",
   "metadata": {},
   "source": [
    "# **MODEL INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d4efbcd-e6ad-4394-b860-bbbcea741831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(input_data: pd.DataFrame) -> np.ndarray:\n",
    "    \n",
    "    dataset_for_testing = r\"C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/data/test.csv\"\n",
    "    scaler_location = r'C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/models/scaler.joblib'\n",
    "    encoder_location = r'C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/models/encoder.joblib'\n",
    "    model_location = r'C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/models/model.joblib'\n",
    "\n",
    "    model = joblib.load(model_location)\n",
    "    Testing_df = pd.read_csv(dataset_for_testing)\n",
    "    scaler = joblib.load(scaler_location)\n",
    "    encoder = joblib.load(encoder_location)\n",
    "    \n",
    "    test_scaled = scaler.transform(Testing_df[continuos_datatype_features])\n",
    "    test_encoded = encoder.transform(Testing_df[discrete_datatype_features])\n",
    "    \n",
    "    test_scaled_df = pd.DataFrame(test_scaled,columns=continuos_datatype_features)\n",
    "    test_encoded_df = pd.DataFrame(test_encoded,columns=encoder.get_feature_names_out(discrete_datatype_features))\n",
    "    \n",
    "    Transformed_test_df = pd.concat([test_scaled_df,test_encoded_df], axis=1)\n",
    "    \n",
    "\n",
    "    predict_house_price = model.predict(Transformed_test_df)\n",
    "    return predict_house_price\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ca3db35-912a-4c1f-99b6-fe9ee6287402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([108263.36369508, 152556.57524383, 185285.93541467, ...,\n",
       "       157936.73796874, 141068.10794168, 197121.11367985])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_for_testing = r\"C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/data/test.csv\"\n",
    "Testing_df = pd.read_csv(dataset_for_testing)\n",
    "make_predictions(Testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb0d08-576b-435a-bc64-4a4df1fbed85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
