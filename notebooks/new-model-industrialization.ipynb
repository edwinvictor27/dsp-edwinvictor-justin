{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c1e97da-4111-4944-9265-b7adb401e311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: joblib in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (2.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pyarrow in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pyarrow) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55b8e19d-cedb-4f62-8e98-e8e0de2be58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13100cbf-f29c-4d91-a522-c564d419a9bb",
   "metadata": {},
   "source": [
    "# **Model building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7eb8b35c-5992-4fcf-9855-005aac4f0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to Load a dataset from the specified file path in the local device.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path of the dataset file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset is loaded as Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded from {file_path} with shape {dataset.shape}\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10ce24cf-f0b9-47f9-9684-eddd09b1cc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/data/train.csv with shape (1460, 81)\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/data/train.csv\"\n",
    "dataset = load_dataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04df8772-7567-4cd1-a941-20ed80627993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(data: pd.DataFrame, selected_features: list, target_feature: str) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    selecting the features and target variable for ml training.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset to prepare features from.\n",
    "        selected_features (list): List of feature names to include.\n",
    "        target_feature (str): The name of the target feature.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.Series]: The selected features and target variable.\n",
    "    \"\"\"\n",
    "    X = data[selected_features]\n",
    "    y = data[target_feature]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab500bfb-7a09-46e7-8501-f3ca932f1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['LotArea', 'GrLivArea', 'Neighborhood', 'HouseStyle']\n",
    "target_feature = ['SalePrice']\n",
    "\n",
    "X, y = select_features(dataset, selected_features, target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5096246-adbf-43ee-866d-1915e2aa317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data: pd.DataFrame, target_feature: str, test_size: float = 0.35, random_state: int = 50) -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    This function is used to Split the dataset into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset to split.\n",
    "        target_feature (str): The column_name of the target feature.\n",
    "        test_size (float): The proportion of the dataset to include in the test split. Defaults to 0.35.\n",
    "        random_state (int): Seed used by the random number generator. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: The training features, testing features, training target, and testing target.\n",
    "    \"\"\"\n",
    "    X = data.drop(target_feature, axis=1)\n",
    "    y = data[target_feature]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e9d7979-b16b-4e39-8c11-6ce39dda14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(dataset, target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f1bdf42-a35e-46dd-9ed9-00a279b53881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X: pd.DataFrame, y: pd.Series, selected_features: list) -> pd.DataFrame:\n",
    "    return X[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "131307a6-3efb-4906-befb-6e3d74cf2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = extract_features(X, y, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e1afd7b-18e3-4ba9-912d-381200376aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_features(X_train: pd.DataFrame, y_train: pd.Series, select_features: list, target_features: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts selected features and the target attribute from the training data and concatenates them into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The training feature data.\n",
    "        y_train (pd.Series): The training target data.\n",
    "        selected_features (list): List of feature names to include.\n",
    "        target_feature (str): The name of the target feature.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the selected features and the target attribute.\n",
    "    \"\"\"\n",
    "    # Extract the selected features from the training data\n",
    "    extracted_features = X_train[selected_features]\n",
    "    \n",
    "    # Extract the target attribute from the training data\n",
    "    extracted_target = y_train.reset_index(drop=True)\n",
    "    \n",
    "    # Concatenate the extracted features and target attribute into a single DataFrame\n",
    "    training_features_df = pd.concat([extracted_features, y_train], axis=1)\n",
    "    \n",
    "    return training_features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa5c0471-e14a-4700-840e-2fc47a788253",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'SalePrice'\n",
    "training_features_df = prepare_training_features(X_train, y_train, selected_features, target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20c78d02-1074-4884-bef6-cc99b29ade50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(df: pd.DataFrame, categorical_features: list) -> pd.DataFrame:\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    encoder.fit(df[categorical_features])\n",
    "    encoded_categories = encoder.transform(df[categorical_features])\n",
    "    encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_features))\n",
    "    return encoded_df, encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "817f377c-5d5f-4a67-86a8-12dcab306532",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Neighborhood', 'HouseStyle']\n",
    "encoded_df, encoder = encode_categorical_features(X_train, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d138507-fbdc-4d29-997d-8f0b16d7d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_continuous_features(df: pd.DataFrame, continuous_features: list) -> pd.DataFrame:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[continuous_features])\n",
    "    scaled_features = scaler.transform(df[continuous_features])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=continuous_features)\n",
    "    return scaled_df, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e8a109e-4494-4a39-bf2c-44c52894771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuos_features = ['LotArea', 'GrLivArea']\n",
    "scaled_df, scaler = scale_continuous_features(X_train, continuos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "065325f7-9483-4602-af53-cdc124c6f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_features(continuous_features_df: pd.DataFrame, categorical_features_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    concatenated = pd.concat([continuous_features_df, categorical_features_df], axis=1)\n",
    "    return concatenated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e9cf4d0-5291-41e6-a4e6-888700acb1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features_df = concatenate_features(scaled_df, encoded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e75dba8-3a86-4d68-9b6a-b1ee149d9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(combined_features_df: pd.DataFrame, y_train: pd.Series, model_path: str, scaler_path: str, encoder_path: str):\n",
    "    model = LinearRegression()\n",
    "    model.fit(combined_features_df, y_train)\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    joblib.dump(encoder, encoder_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80318537-fdba-4213-873f-2053ef4b3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\edwin victor\\git repositories\\dsp-edwinvictor-justin\\models\\model.joblib\"\n",
    "scaler_path = r\"C:\\Users\\edwin victor\\git repositories\\dsp-edwinvictor-justin\\models\\scaler.joblib\"\n",
    "encoder_path = r\"C:\\Users\\edwin victor\\git repositories\\dsp-edwinvictor-justin\\models\\encoder.joblib\"\n",
    "model = train_model(combined_features_df, y_train, model_path, scaler_path, encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2741977b-7d83-46b4-9eb1-a5fc4d28c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_testing_set(X_test: pd.DataFrame, categorical_features: list, continuous_features: list, encoder, scaler) -> pd.DataFrame:\n",
    "    # Encode categorical features\n",
    "    \n",
    "    encoder = joblib.load(encoder)\n",
    "    scaler = joblib.load(scaler)\n",
    "    categorical_df = X_test[categorical_features]\n",
    "    continuous_df = X_test[continuous_features]\n",
    "\n",
    "    encoded_categories = encoder.transform(categorical_df)\n",
    "    encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_features))\n",
    "    \n",
    "    # Scale continuous features\n",
    "    scaled_features = scaler.transform(continuous_df)\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=continuous_features)\n",
    "    \n",
    "    # Concatenate features\n",
    "    processed_test_df = pd.concat([scaled_df, encoded_df], axis =1)\n",
    "    y_pred = model.predict(processed_test_df)\n",
    "    return processed_test_df,y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d139dcbb-5088-493e-b884-111700cb2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set, y_pred = process_testing_set(X_test, categorical_features,continuos_features,encoder_path,scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27de0850-7fb3-4e2b-9cc6-445eaa9b0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test: pd.Series, y_pred: np.ndarray) -> dict[str, str]:\n",
    "    Rmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
    "    return {'Root Mean Squared Error out of': str(Rmsle)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04dbd861-697b-4add-82e5-511a6050d95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Root Mean Squared Error out of': '0.19120286564820066'}\n"
     ]
    }
   ],
   "source": [
    "evaluate = evaluate_model(y_test, y_pred)\n",
    "print(evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bddd79-bdaf-46d4-9da4-f47b4e253bf0",
   "metadata": {},
   "source": [
    "# **Model Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e31772e8-0d22-4e35-bf68-b4f9880dd025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_and_transformers(model_path: str, scaler_path: str, encoder_path: str):\n",
    "    \"\"\"\n",
    "    Loads the pre-trained model, scaler, and encoder.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved model.\n",
    "        scaler_path (str): Path to the saved scaler.\n",
    "        encoder_path (str): Path to the saved encoder.\n",
    "\n",
    "    Returns:\n",
    "        model: Loaded model object.\n",
    "        scaler: Loaded scaler object.\n",
    "        encoder: Loaded encoder object.\n",
    "    \"\"\"\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    encoder = joblib.load(encoder_path)\n",
    "    return model, scaler, encoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "982d5455-b2c4-4434-ad6c-ae7fdb705d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, scaler, encoder = load_model_and_transformers(model_path, scaler_path, encoder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b77208c9-a0e9-4eae-8fa5-f84651876fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_and_predict(Testing_df: pd.DataFrame, scaler, encoder, model, continuos_features: list, categorical_features: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the input data by scaling continuous features and encoding categorical features.\n",
    "\n",
    "    Args:\n",
    "        input_data (pd.DataFrame): The data to preprocess.\n",
    "        scaler: Scaler object to scale continuous features.\n",
    "        encoder: Encoder object to encode categorical features.\n",
    "        continuous_features (list): List of continuous feature names.\n",
    "        discrete_features (list): List of discrete feature names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed data.\n",
    "    \"\"\"\n",
    "    test_scaled = scaler.transform(Testing_df[continuos_features])\n",
    "    test_encoded = encoder.transform(Testing_df[categorical_features])\n",
    "\n",
    "    test_scaled_df = pd.DataFrame(test_scaled, columns=continuos_features)\n",
    "    test_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "    transformed_test_df = pd.concat([test_scaled_df, test_encoded_df], axis=1)\n",
    "    predict_house_price = model.predict(transformed_test_df)\n",
    "    return predict_house_price\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77841229-6421-45c4-a808-0fff84c7c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[111539.65323364]\n",
      " [153677.46598706]\n",
      " [185488.24754898]\n",
      " ...\n",
      " [157965.97784075]\n",
      " [144389.60544552]\n",
      " [197115.00156427]]\n"
     ]
    }
   ],
   "source": [
    "Test_dataset = r\"C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/data/test.csv\"\n",
    "Testing_df = pd.read_csv(Test_dataset)\n",
    "predict_house_price = preprocess_data_and_predict(Testing_df, scaler, encoder, model ,continuos_features, categorical_features)\n",
    "print(predict_house_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afababe3-d328-4e90-bc66-d423a90666bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
