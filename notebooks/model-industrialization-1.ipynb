{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d392c71-4f5b-411f-a852-e7c3464a9900",
   "metadata": {},
   "source": [
    "# **Installing Pre-requisites for our Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aa9bf0c-052a-4e5d-9f7b-613a6f051d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (2.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pyarrow in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (from pyarrow) (2.0.0)\n",
      "Requirement already satisfied: joblib in d:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install pyarrow\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8986c1be-e617-418d-860c-60a18b195435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9351599-8e89-49ae-a8c5-a78c1e166347",
   "metadata": {},
   "source": [
    "# **MODEL BUILDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a842db1b-1e48-4846-b25e-b9659b6a7466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv file using read_csv function\n",
    "dataset_for_training = r\"C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/data/train.csv\"\n",
    "Training_dataset = pd.read_csv(dataset_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd2c66e5-fe71-463a-ab17-fdbd97a7c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring feature selection variables outside incase if i want to change the features i can do it just here\n",
    "\n",
    "selected_features = ['LotArea', 'GrLivArea', 'Neighborhood', 'HouseStyle'] # 2 continuos and 2 Categorical features\n",
    "target_feature = ['SalePrice']\n",
    "continuos_datatype_features = ['LotArea', 'GrLivArea']\n",
    "discrete_datatype_features = ['Neighborhood', 'HouseStyle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ba5222-fddc-48cc-bb8f-0a5ef764b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(data: pd.DataFrame) -> dict[str, str]:\n",
    "\n",
    "## Training_set\n",
    "    \n",
    "    # 1) splitting the dataset\n",
    "    \n",
    "    X = Training_dataset.drop(target_feature, axis=1)\n",
    "    y = Training_dataset[target_feature]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=50)\n",
    "\n",
    "\n",
    "    # 2) Extracting the features from training set\n",
    "    \n",
    "    Extracted_Selected_Features_For_Training = X_train[selected_features]\n",
    "    Extracted_Target_Attribute = y_train[target_feature]\n",
    "    Training_Features = pd.concat([Extracted_Selected_Features_For_Training,y_train], axis=1)\n",
    "\n",
    "    # 3) Encoding the categorical columns from training set\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output= False)\n",
    "    encoder.fit(Training_Features[discrete_datatype_features])\n",
    "    Training_encoded_categories = encoder.transform(Training_Features[discrete_datatype_features])\n",
    "    encoded_discrete_features_training_df = pd.DataFrame(Training_encoded_categories, columns=encoder.get_feature_names_out(discrete_datatype_features))\n",
    "\n",
    "    # 4) Scaling the continuos columns from training set\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[continuos_datatype_features])\n",
    "    scaled_continuos_features_training_df = scaler.transform(X_train[continuos_datatype_features])\n",
    "\n",
    "    # 5) Concatenating the processed training set\n",
    "    \n",
    "    training_continuous_features_df = pd.DataFrame(scaled_continuos_features_training_df , columns= continuos_datatype_features)\n",
    "    Processed_Training_Df = pd.concat([training_continuous_features_df, encoded_discrete_features_training_df] , axis=1)\n",
    "\n",
    "    # 6) Fitting the model\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(Processed_Training_Df, y_train)\n",
    "\n",
    "## Testing_set   \n",
    "\n",
    "    # 1) Extracting the features from testing set\n",
    "    \n",
    "    Extracted_Features_Testing = X_test[selected_features]\n",
    "    Testing_Features = pd.concat([Extracted_Features_Testing,y_test], axis=1)\n",
    "\n",
    "    # 2) Encoding the categorical columns from testing set\n",
    "    \n",
    "    encoder.fit(Testing_Features[discrete_datatype_features])\n",
    "    Testing_encoded_categories = encoder.transform(Testing_Features[discrete_datatype_features])\n",
    "    encoded_discrete_features_testing_df = pd.DataFrame(Testing_encoded_categories, columns=encoder.get_feature_names_out(discrete_datatype_features))\n",
    "\n",
    "    # 3) Scaling the continuos columns from testing set\n",
    "    \n",
    "    scaler.fit(X_test[continuos_datatype_features])\n",
    "    scaled_continuos_features_testing_df = scaler.transform(X_test[continuos_datatype_features])\n",
    "\n",
    "    # 4) Concatenating the processed testing set\n",
    "    \n",
    "    testing_continuous_features_df = pd.DataFrame(scaled_continuos_features_testing_df , columns= continuos_datatype_features)\n",
    "    Processed_Testing_Df = pd.concat([testing_continuous_features_df, encoded_discrete_features_testing_df] , axis=1)\n",
    "    \n",
    "    # 5) Making prediction \n",
    "    \n",
    "    y_pred = model.predict(Processed_Testing_Df)\n",
    "\n",
    "    # 6) Evaluating the model\n",
    "    \n",
    "    Rmsle = np.sqrt(mean_squared_log_error(y_test, y_pred)) \n",
    "    return {'Root Mean Squared Error out of': str(Rmsle) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87b2e349-dac1-4804-9a14-90bcce4bd05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Root Mean Squared Error out of': '0.19357831076666'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_model(Training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d01444-eca5-402b-bd8a-aaa36bfb829c",
   "metadata": {},
   "source": [
    "# **MODEL INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d4efbcd-e6ad-4394-b860-bbbcea741831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(input_data: pd.DataFrame) -> np.ndarray:\n",
    "    \n",
    "    dataset_for_testing = r\"C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/data/test.csv\"\n",
    "    scaler_location = r'C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/models/scaler.joblib'\n",
    "    encoder_location = r'C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/models/encoder.joblib'\n",
    "    model_location = r'C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/models/model.joblib'\n",
    "\n",
    "    \n",
    "    model = joblib.load(model_location)\n",
    "    Testing_df = pd.read_csv(dataset_for_testing)\n",
    "    scaler = joblib.load(scaler_location)\n",
    "    encoder = joblib.load(encoder_location)\n",
    "    \n",
    "    test_scaled = scaler.transform(Testing_df[continuos_datatype_features])\n",
    "    test_encoded = encoder.transform(Testing_df[discrete_datatype_features])\n",
    "    \n",
    "    test_scaled_df = pd.DataFrame(test_scaled,columns=continuos_datatype_features)\n",
    "    test_encoded_df = pd.DataFrame(test_encoded,columns=encoder.get_feature_names_out(discrete_datatype_features))\n",
    "    \n",
    "    Transformed_test_df = pd.concat([test_scaled_df,test_encoded_df], axis=1)\n",
    "    \n",
    "\n",
    "    predict_house_price = model.predict(Transformed_test_df)\n",
    "    return predict_house_price\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ca3db35-912a-4c1f-99b6-fe9ee6287402",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset_for_testing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/data/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m Testing_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_for_testing)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmake_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTesting_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m, in \u001b[0;36mmake_predictions\u001b[1;34m(input_data)\u001b[0m\n\u001b[0;32m     11\u001b[0m scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(scaler_location)\n\u001b[0;32m     12\u001b[0m encoder \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(encoder_location)\n\u001b[1;32m---> 14\u001b[0m test_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTesting_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontinuos_datatype_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m test_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(Testing_df[discrete_datatype_features])\n\u001b[0;32m     17\u001b[0m test_scaled_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test_scaled,columns\u001b[38;5;241m=\u001b[39mcontinuos_datatype_features)\n",
      "File \u001b[1;32mD:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mD:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1042\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform standardization by centering and scaling.\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \n\u001b[0;32m   1030\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m        Transformed array.\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m     copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m   1045\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1046\u001b[0m         X,\n\u001b[0;32m   1047\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1052\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1053\u001b[0m     )\n",
      "File \u001b[1;32mD:\\miniconda\\miniconda\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py:1661\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "dataset_for_testing = r\"C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/data/test.csv\"\n",
    "Testing_df = pd.read_csv(dataset_for_testing)\n",
    "make_predictions(Testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ddb0d08-576b-435a-bc64-4a4df1fbed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_location = r'C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/models/scaler.joblib'\n",
    "encoder_location = r'C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/models/encoder.joblib'\n",
    "model_location = r'C:/Users/edwin victor/git repositories/dsp-edwinvictor-justin/models/model.joblib'\n",
    "\n",
    "model = LinearRegression()\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "saving_model_object = joblib.dump(model,model_location)\n",
    "saving_scaler_object = joblib.dump(scaler,scaler_location)\n",
    "saving_encoder_object = joblib.dump(encoder,encoder_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea514c-2e17-40d1-b0df-50678cca874c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375d037-d3a9-47f0-b39f-85bacb7b1616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940a24e-08a3-4601-9005-a78f5373fead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c1196-45f7-4e21-997f-0cb6ca6deb79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
